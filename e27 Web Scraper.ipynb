{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lists to store the relevant information (company name, location, market size, funding and website URL) \n",
    "to be extracted from the website.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyNameList = []\n",
    "locationList = []\n",
    "marketList = []\n",
    "fundingList = []\n",
    "websiteList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializing an empty string for the purpose of storing multiple lines of information about the same company.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempString = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To define the number of website pages for data extraction. At the time of creation, there were 1265 pages \n",
    "available for extraction.\n",
    "A small number is used initially to test the validity of the code.\n",
    "As a gauge, it took about 15 minutes to extract all 1265 pages of information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minPage = 1\n",
    "maxPage = 6     #Max pages available is 1265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section of code here loops through all the available pages of information from the website individually \n",
    "to obtain the json response, cleans the data by removing the unnecessary whitespaces before \n",
    "splitting the data into smaller chunks based on observed patterns to facilitate theinformation extraction process. \n",
    "Data extracted is then sorted before output into a csv file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(minPage,maxPage):    \n",
    "\n",
    "    # Getting the AJAX url call as the unlimited scrolling webpage loads\n",
    "    url = \"https://e27.co/startups/load_startups_ajax?all&per_page=\" + str(i) + \"&append=1&_=2017-07-15_23:19:03_03\"\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    rawData = r.content\n",
    "\n",
    "    # Removing the unnecessary whitespaces\n",
    "    for ch in ['/','\\\\t','\\\\n']:\n",
    "        if ch in rawData:\n",
    "            rawData = rawData.replace(ch, '')\n",
    "\n",
    "    # Replacing the ampersand character within the raw data\n",
    "    rawData = rawData.replace('&amp', '&')\n",
    "\n",
    "    # Splitting the raw data based on observed patterns to facilitate information extraction\n",
    "    rawDataForCompany = rawData.split('h3')\n",
    "\n",
    "    rawDataForOthers = rawData.split('<\\\\div>')\n",
    "\n",
    "    # Extracting the company name\n",
    "    for x in range(1,len(rawDataForCompany),2):\n",
    "            companyNameList.append(rawDataForCompany[x][24:-2])\n",
    "\n",
    "    # Extracting the location\n",
    "    for x in range(3,len(rawDataForOthers),16):\n",
    "            locationList.append(rawDataForOthers[x][21:])\n",
    "\n",
    "    # Extracting the market size\n",
    "    for x in range(6,len(rawDataForOthers),16):\n",
    "        for y in range(2,len(rawDataForOthers[x].split('>')),4):\n",
    "            tempString += ((rawDataForOthers[x].split('>')[y][:-3])+ ' ')\n",
    "        marketList.append(tempString)\n",
    "        tempString = ''\n",
    "\n",
    "    # Extracting the website URL\n",
    "    for x in range(9,len(rawDataForOthers),16):\n",
    "            websiteList.append(rawDataForOthers[x].split('\"')[-2])\n",
    "\n",
    "    # Extracting the size of funding obtained\n",
    "    for x in range(12,len(rawDataForOthers),16):\n",
    "            fundingList.append(rawDataForOthers[x][21:])\n",
    "\n",
    "    # To provide a visual cue to the progress of the extraction\n",
    "    if i == int(maxPage * 0.25):\n",
    "        print(\"25% completed.\")\n",
    "    elif i == int(maxPage * 0.5):\n",
    "        print(\"50% completed.\")\n",
    "    elif i == int(maxPage * 0.75):\n",
    "        print(\"75% completed.\")\n",
    "    elif i == int(maxPage):\n",
    "        print(\"Scraping completed.\")\n",
    "\n",
    "# To match the company to the various information extracted to ensure consistency\n",
    "columns = zip(companyNameList,locationList,marketList,websiteList,fundingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output the information extracted to a csv file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing data to csv file.\")\n",
    "\n",
    "with open('test.csv', 'w') as outfile:\n",
    "    c = csv.writer(outfile)\n",
    "    for column in columns:\n",
    "        c.writerow(column)\n",
    "    \n",
    "print(\"Process completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
